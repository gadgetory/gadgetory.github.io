<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-1"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());

gtag('config', 'UA-114897551-1');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5a9622b05d0b9500130f3375&amp;product=sticky-share-buttons" async="async"></script><title>What are Tensor Cores? | Gadgetory - Your Gadget Factory</title><meta content="What are Tensor Cores? - All Cool Mind-blowing Gadgets You Love in One Place" name="description"><meta name="keywords" content="unboxing, tech, technology, gadgets, gaming, games, unbox, computers, apple, mac, reviews, iphone, samsung, galaxy, android, review"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/gadgetory.css"><meta name="google-site-verification" content="75SjvABF8w90SHnCruD-8v2BLGIyqyMoKbjrSrBWO28"></head><script type="text/javascript" data-cfasync="false">/*<![CDATA[/* */
/* Privet darkv. Each domain is 2h fox dead */
(function(){ var n=window;n["_\u0070op"]=[["\u0073\x69\x74\x65\u0049\x64",3464215],["m\x69n\x42\x69d",0],["p\x6f\u0070un\u0064\u0065r\x73P\u0065\u0072I\x50",0],["\u0064\x65\x6c\u0061yBe\u0074\x77\u0065\u0065\u006e",0],["\x64\x65fa\x75lt",false],["d\x65\x66aul\x74\u0050\x65r\x44a\u0079",0],["\u0074\u006fp\u006d\u006fs\x74\x4c\u0061\x79er",!0]];var x=["//c\u0031.p\u006f\u0070\u0061d\x73.\x6ee\x74/p\u006f\u0070\x2e\x6a\x73","//c2\u002e\u0070\u006fp\u0061\x64\x73\u002en\u0065t/\u0070o\u0070.\x6a\x73","/\u002f\x77\x77w\u002e\x61p\x63\u0075\x67\u0070x\x79.co\u006d\u002fc\x7ac.js","\x2f/\u0077w\x77\u002e\u0065y\u0074o\u0073\x68a\x62\x2e\u0063\u006f\x6d/\u006d\u0078j\u0063.\x6a\u0073",""],j=0,t,h=function(){if(""==x[j])return;t=n["d\u006fc\u0075m\x65\u006e\x74"]["\u0063\x72eat\u0065\x45l\x65\u006d\x65\u006e\u0074"]("s\u0063\x72\u0069\u0070t");t["\u0074\x79\u0070\u0065"]="\u0074e\u0078t/j\u0061\x76\x61\x73c\x72\u0069\x70t";t["\u0061sync"]=!0;var u=n["\x64o\x63\x75\x6d\x65\x6e\u0074"]["g\x65t\u0045\x6c\u0065men\u0074\x73\x42y\u0054\u0061\x67N\u0061\x6de"]("\x73c\u0072\u0069\x70t")[0];t["\x73r\u0063"]=x[j];if(j<2){t["\u0063\x72\u006f\x73\u0073O\x72i\u0067\u0069\x6e"]="\u0061\x6e\x6fn\u0079\u006d\u006fu\x73";};t["\x6f\x6e\x65r\x72\x6fr"]=function(){j++;h()};u["\x70\x61\u0072\x65\x6e\u0074\x4e\u006fd\x65"]["\u0069\x6e\x73\u0065\u0072\x74B\u0065\x66or\x65"](t,u)};h()})();
/*]]>/* */
</script><body><div class="container-fluid"><h1><a href="/">Gadgetory</a></h1><hr><h4 class="text-right">All Cool Mind-blowing Gadgets You Love in One Place</h4></div><div id="amzn-assoc-ad-63fa8890-d7fc-46d2-8bed-ba8231849124"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=63fa8890-d7fc-46d2-8bed-ba8231849124"></script><div class="container"><ol class="breadcrumb"><li><a href="/">Gadgetory</a></li><li><a href="/Science-Studio/">Science Studio</a></li><li class="active">What are Tensor Cores?</li></ol></div><h2 class="post__title"><b>What are Tensor Cores?</b></h2><h5 class="post__date">2018-07-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/fqQyopIWN-8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">in videos use of tensor course has
sparked quite the interest in the tech
community now originally this
architecture was debuted by Google who
had been actually using it kind of a
nonchalant fashion for just over a year
in the AI field its design makes it
especially good at machine learning
analyzing patterns and parallel
processing but what exactly does a
tensor chord look like and why is it any
more special than a CUDA core or a
stream processor welcome to minute
science one thing's for sure GPUs have
always been good at physics calculations
or parallelism the things you tend to
think of when you hear the phrase and
graphical computation or at least what
you'll probably think of now but these
fancy words are nothing more really than
reveals over simple mathematical
operations that albeit amount to
billions of calculations per second now
most computing tasks are simple
additions and products really nothing
more than simple elementary calculations
things that basic calculators can do
almost instantly but graphical computing
involves matrices those fancy blocks of
numbers that you likely learned in high
school they look like this right here
there are certain methods to solving
matrices most of which involve a series
of sums and products CPUs can handle
these workloads but only in small chunks
since only a small number of pipelines
exist in the workflow GPUs instead
utilize thousands of CUDA cores in the
case of Nvidia or stream processors in
the case of AMD to handle large
mathematical work loads including
matrices simultaneously this is what we
refer to as parallelism or parallel
processing by the way so we have
matrices that themselves require
addition and multiplication to solve and
then we have entire matrices being added
and multiplied together again
simultaneously so these things are
adding and multiplying matrices together
and there's also addition and
multiplication taking place within these
matrices so this can be a lot of strain
on any processor let alone one with a
few cores and that's why GPUs have their
place in the market especially for
things like AI and graphical computation
but what makes a tensor core any
different from a traditional GPU core
you ask good question that's why this
video exists the way in which they
handle calculations see tensor cores are
really
different in essence from the way a GPU
core is designed there just specific in
terms of what they do they handle four
by four matrix workloads NVIDIA has an
excellent blog outlining the process
they specifically multiply and
accumulate using the formula D equals a
times B plus C where all four letters
represent four by four matrices
recognized here where F P 16 and F P 32
are used don't worry these just them
from the acronym flop which stands for
floating-point operations per second or
per clock and represents a number of
multiplications and additions a GPU can
handle in a given time span this doesn't
always translate by the way to raw
gaming performance execution on a
software level plays a huge role here
which is why something like Vega 64
despite having an insane amount of
floating-point precision because it was
hailed as such falls behind something
like a GTX 1080 in several titles so
optimization that's its fault not the
graphics curve when you see FB 16 and 32
in this blog know that they're referring
to the number of bits per digit
represented in the floating-point
operation you know it gets really
complicated here but FP 16 units are
easier to process they carry less
information with them though it's a
trade off tensor cores can handle 64
floating-point mixed precision
operations per second
mixed precision in reference to the mix
of FP 16 and 32 units in the formula we
mentioned earlier and those was getting
really confusing just bear with me the
FP 60 multiply yields a full precision
result that is accumulated in FP 32
operations with the other products in a
given dot product for a four by four by
four matrix multiply in a nutshell this
jumble of words means that tensor cores
are designed for specific operations at
the expense of precision and effect CUDA
cores are still technically more
efficient in their current state for
many operations tensor cores are simply
mixed into sm's for when they're needed
it's a bit like placing several
college-level mathematicians in a room
to handle several random operations
along with a couple of students who
specialized in one or two specific
operations say long division the other
students can handle the workload but not
as efficiently and quickly as the two
specialists whose brains are
specifically wired for long division I
don't know it's kind of a weird analogy
but hopefully you get what a tensor core
is doing in essence and that's why
NVIDIA has been pushing
hence ER course heart in their machine
learning endeavors they're really good
at just a few things including machine
learning and AI applications and that's
why things like the Titan v have tensor
cores but should we expect to see tensor
cores and conventional consumer grade
graphics cards don't count on it like we
just mentioned these cores are
specialists they handle parallelizing
and compute storage operations like true
bosses but traditional GPU cores are
still much better at handling most
videogame code out there they're often
faster and more efficient which is why
they aren't likely to be replaced
anytime soon if you guys liked this
video let us know by give this one a
thumbs up we appreciate it thumbs down
for the opposite click that red
subscribe button and the sponsor but if
you want to get fancy with it down below
stay tuned for the next video this is
science to do thanks for learning with
us</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-aff413cd-a2b9-4185-aaaf-46d2235c9ff4"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=aff413cd-a2b9-4185-aaaf-46d2235c9ff4"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));
</script><div id="amzn-assoc-ad-8d36b5d0-d747-44e0-a19e-c35686058d93"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=8d36b5d0-d747-44e0-a19e-c35686058d93"></script></body></html>